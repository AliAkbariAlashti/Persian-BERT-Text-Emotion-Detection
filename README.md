ğŸ“– Persian BERT Text Classification

A complete end-to-end student project: dataset â†’ fine-tuning â†’ evaluation â†’ Hugging Face Hub

âœ¨ Project Overview

This project demonstrates how to fine-tune a Persian BERT model for text classification (e.g., sentiment/emotion analysis).
It is designed as a student-friendly tutorial, showing the complete workflow from data preparation to model deployment.

With this tutorial, you will learn how to:

âœ… Load and preprocess Persian text data

âœ… Encode labels for classification

âœ… Fine-tune ParsBERT (HooshvareLab/bert-fa-zwnj-base) on your dataset

âœ… Evaluate your model with accuracy, precision, recall, F1

âœ… Visualize results with confusion matrix & confidence plots

âœ… Save and upload your model to the Hugging Face Hub

âœ… Run inference with new Persian sentences

ğŸ› ï¸ Requirements

Install dependencies:

pip install transformers datasets accelerate evaluate scikit-learn seaborn huggingface_hub

ğŸ“‚ Project Structure
persian-bert-text-classification/
â”‚â”€â”€ dataset.csv              # Your dataset (text + label)
â”‚â”€â”€ training_script.ipynb    # Main Colab/Jupyter script
â”‚â”€â”€ results/                 # Training outputs (checkpoints, logs)
â”‚â”€â”€ persian-bert-emotion/    # Saved fine-tuned model

ğŸ“Š Dataset Format

Your dataset must be a CSV file with two columns:

text	label
Ø§Ù…Ø±ÙˆØ² Ø±ÙˆØ² Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª	positive
Ù…Ù† Ø®ÛŒÙ„ÛŒ Ù†Ø§Ø±Ø§Ø­ØªÙ…	negative
Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯	positive
Ú†Ù‚Ø¯Ø± Ø®Ø³ØªÙ‡â€ŒØ§Ù…	negative
ğŸš€ Training

Run the notebook step by step:

Setup (libraries, GPU check, seed)

Load Dataset (CSV file with text + label)

Preprocessing (normalize Persian text)

Tokenization (ParsBERT tokenizer)

Model Setup (classification head + label mapping)

Training (fine-tune with Trainer)

Evaluation (metrics + visualization)

Save & Upload (to Hugging Face Hub)

Inference Demo (test sentences in Persian)

ğŸ“ˆ Evaluation Visuals

The notebook generates:

ğŸ¯ Confusion Matrix

ğŸ“Š F1-Score per class

ğŸ“ˆ Precision vs Recall scatter plot

ğŸ” Confidence distribution (correct vs incorrect predictions)

ğŸŒ Upload to Hugging Face Hub

Login to your Hugging Face account:

from huggingface_hub import notebook_login
notebook_login()


Then push your model:

trainer.push_to_hub("your-username/persian-bert-emotion-classification")

ğŸ”® Inference Example
from transformers import pipeline
classifier = pipeline("text-classification", model="your-username/persian-bert-emotion-classification")

text = "Ø§Ù…Ø±ÙˆØ² Ø±ÙˆØ² ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø´ØªÙ…"
print(classifier(text))


Output:

[{'label': 'positive', 'score': 0.95}]

ğŸ“Œ Next Steps

Try different models (XLM-R, mBERT)

Experiment with longer training & hyperparameters

Collect more Persian data for better performance

Deploy with Gradio or Streamlit

ğŸ™Œ Acknowledgments

ParsBERT (HooshvareLab)

Hugging Face Transformers

âœ¨ Happy Fine-tuning! âœ¨
